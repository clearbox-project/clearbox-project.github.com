<article class="note">
  <h2 class="note-title">
    Perplexity AI The Future of Intelligence (highlight)
  </h2>
  <div class="content"><h1 id="perplexity-ai-the-future-of-intelligence-highlight">Perplexity AI: The Future of Intelligence (highlight)</h1>

<p><img src="https://digitalbeech.com/wp-content/uploads/2022/12/What-is-Perplexity_11zon.png" alt="rw-book-cover" /></p>

<h2 id="metadata">Metadata</h2>

<ul>
<li>Author: <a href="princeji.html">Princeji</a></li>
<li>Full Title: Perplexity AI: The Future of Intelligence</li>
<li>Category: <span class="tag">articles</span></li>
<li>URL: https://digitalbeech.com/perplexity-ai/</li>
</ul>

<h2 id="highlights">Highlights</h2>

<ul>
<li>Perplexity is typically calculated by dividing the exponentiated average negative log probability of the test set by the number of words in the test set. In other words, it is a measure of the model’s uncertainty or <strong>confusion when predicting the next word in a sequence. The lower the perplexity, the better the model is at predicting the next word, and vice versa.</strong> (<a href="https://read.readwise.io/read/01grhzvv6zxrtjy88t5cnqtmfk">View Highlight</a>)</li>
<li>• <strong>BLEU (bilingual evaluation understudy)</strong>: BLEU is a metric for evaluating the performance of machine translation models, and is based on the idea of comparing the model’s output to a reference translation. It is calculated by counting the number of words that overlap between the model’s output and the reference translation, and is typically used to evaluate the overall accuracy and fluency of machine translation models.<br />
• <strong>ROUGE (recall-oriented understudy for gisting evaluation)</strong>: ROUGE is a metric for evaluating the performance of text summarization models, and is based on the idea of comparing the model’s output to a reference summary. It is calculated by counting the number of words that overlap between the model’s output and the reference summary, and is typically used to evaluate the overall coherence and relevance of text summaries.<br />
• <strong>F1 score</strong>: The F1 score is a metric for evaluating the performance of classification models, and is based on the idea of balancing precision and recall. It is calculated by taking the harmonic mean of precision and recall, and is typically used to evaluate the overall accuracy and effectiveness of classification models.<br />
• <strong>Accuracy</strong>: Accuracy is a simple and widely used metric for evaluating the performance of NLP models, and is calculated by dividing the number of correct predictions made by the model by the total number of predictions. Accuracy is often used as a baseline metric for evaluating the performance of NLP models, but may not always be the best metric for more complex tasks. (<a href="https://read.readwise.io/read/01grj21sgrpspj477f1qya3089">View Highlight</a>)</li>
</ul>

<p><span class="timestamp">Last update: 02/06/2023 15:50</span></p>
</div>
</article>

