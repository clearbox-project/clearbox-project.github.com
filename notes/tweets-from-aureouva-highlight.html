<article class="note">
  <h2 class="note-title">
    Tweets From Aureouva (highlight)
  </h2>
  <div class="content"><h1 id="tweets-from-aureouva-highlight">Tweets From Aureouva (highlight)</h1>

<p><img src="https://pbs.twimg.com/profile_images/1348270568907280384/eryF3Bc8.jpg" alt="rw-book-cover" /></p>

<h2 id="metadata">Metadata</h2>

<ul>
<li>Author: <a href="aureouva-on-twitter.html">@aureouva on Twitter</a></li>
<li>Full Title: Tweets From Aureouva</li>
<li>Category: <span class="tag">tweets</span></li>
<li>URL: https://twitter.com/aureouva</li>
</ul>

<h2 id="highlights">Highlights</h2>

<ul>
<li>GPT应该是要发生在不同级别上，就像分形。最基础的GPT是以token为单位进行注意力机制的token选择，然后送入生成器。<br />
这时候的问题就是对上下文的长度限制，人们想喂更长的语料。<br />
于是第二等级应该是以句子或者段落为单位算embedding vector，再进行第二等级的注意力vector选择。 (<a href="https://twitter.com/aureouva/status/1633123268537516033">View Tweet</a>)</li>
</ul>

<p><span class="timestamp">Last update: 03/09/2023 07:19</span></p>
</div>
</article>

